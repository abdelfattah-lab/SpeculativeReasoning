import random
import argparse
import numpy as np
import matplotlib.pyplot as plt

def simulate_inference_time(
    p_t=2048,              # Prefill length
    d_t=16384,             # Decode length
    big_pre=1000.0,        # Big model prefill rate (tokens/s)
    big_dec=500.0,         # Big model decode rate  (tokens/s)
    small_pre=2000.0,      # Small model prefill rate (tokens/s)
    small_dec=1500.0,      # Small model decode rate (tokens/s)
    big_coverage=0.1,      # Fraction of decode tokens covered by the big model
    chunk_size=256         # Size of each chunk
):
    """
    Simulate total inference time (in seconds) for a two-model approach:
      - Both models get prefills in parallel, so prefill time = max(time_big_prefill, time_small_prefill).
      - The decode is split into chunks; each chunk is either generated by small or big model,
        while the other model is simultaneously "prefilling" that chunk.
      - We randomly position enough <bigmodel> segments so that ~`big_coverage` fraction
        of the total decode tokens are done by the big model.
    """
    # 1) Prefill Phase (parallel)
    time_big_prefill = p_t / big_pre
    time_small_prefill = p_t / small_pre
    prefill_time = max(time_big_prefill, time_small_prefill)

    # 2) Decode Phase
    num_chunks = d_t // chunk_size
    leftover = d_t % chunk_size
    partial_chunk = (leftover > 0)

    # Determine how many total tokens should be "big" based on big_coverage
    total_big_tokens = int(round(big_coverage * d_t))

    # Big coverage in full chunks
    big_chunks_needed = total_big_tokens // chunk_size
    leftover_for_big = total_big_tokens % chunk_size

    # We can't exceed total full chunks
    big_chunks_needed = min(big_chunks_needed, num_chunks)

    # Create list of chunk labels
    chunk_labels = ['small'] * num_chunks
    big_indices = random.sample(range(num_chunks), big_chunks_needed)
    for idx in big_indices:
        chunk_labels[idx] = 'big'

    # Partial chunk label
    partial_chunk_label = 'small'
    if partial_chunk and leftover_for_big > 0:
        # 50-50 chance to assign leftover partial chunk to big
        if random.random() < 0.5:
            partial_chunk_label = 'big'

    # Compute decode time (sum of chunk times, each chunk is max of decode vs the other model's prefill)
    decode_time_sum = 0.0

    # 2a) Full chunks
    for label in chunk_labels:
        if label == 'big':
            dec_t = chunk_size / big_dec
            pre_t = chunk_size / small_pre
        else:
            dec_t = chunk_size / small_dec
            pre_t = chunk_size / big_pre
        chunk_time = max(dec_t, pre_t)
        decode_time_sum += chunk_time

    # 2b) Partial chunk
    if partial_chunk:
        if partial_chunk_label == 'big':
            dec_t = leftover / big_dec
            pre_t = leftover / small_pre
        else:
            dec_t = leftover / small_dec
            pre_t = leftover / big_pre
        decode_time_sum += max(dec_t, pre_t)

    total_time_seconds = prefill_time + decode_time_sum

    # Diagnostics
    total_big_tokens_est = big_chunks_needed * chunk_size
    if partial_chunk and partial_chunk_label == 'big':
        total_big_tokens_est += leftover
    coverage_est = total_big_tokens_est / float(d_t)

    return {
        'total_time_s': total_time_seconds,
        'prefill_time_s': prefill_time,
        'decode_time_s': decode_time_sum,
        'actual_big_tokens': total_big_tokens_est,
        'requested_big_tokens': total_big_tokens,
        'coverage_est': coverage_est,
        'coverage_requested': big_coverage
    }


def run_inverted_scaling_law_plot(
    coverage_start=0.0,
    coverage_end=0.50,
    coverage_step=0.05,
    sims_per_coverage=10,
    p_t=2048,
    d_t=16384
):
    # Prefill / decode rates for known models
    prefill_map = {
        "1.5B": 28868.4664,
        "7B":   7658.083194,
        "8B":   7456.973294,
        "14B":  3835.762802,
        "32B":  2594.868088,
        "70B":  2237.695188
    }
    decode_map = {
        "1.5B": 141,
        "7B":   41.3,
        "8B":   40.3,
        "14B":  22,
        "32B":  16,
        "70B":   9
    }

    # We'll fix the small model to 1.5B, vary the big model
    small_model = "1.5B"
    big_models = ["7B", "8B", "14B", "32B", "70B"]

    coverage_values = np.arange(coverage_start, coverage_end + 1e-9, coverage_step)

    # Optional: choose a style for a "cool" looking plot
    # plt.style.use("ggplot")

    # For a pretty colormap
    cmap = plt.cm.CMRmap
    colors = cmap(np.linspace(0, 1, 2*len(big_models)))

    plt.figure(figsize=(8, 5))

    for color, big_model in zip(colors, big_models):
        # Single-model big-only baseline
        big_pre = prefill_map[big_model]
        big_dec = decode_map[big_model]
        big_only_time = (p_t / big_pre) + (d_t / big_dec)

        # We'll store (speedup, coverage) for each coverage
        coverage_list = []
        speedup_list = []

        for coverage in coverage_values:
            # Run the simulation multiple times to average out randomness
            multi_times = []
            for _ in range(sims_per_coverage):
                sim_result = simulate_inference_time(
                    p_t=p_t,
                    d_t=d_t,
                    big_pre=big_pre,
                    big_dec=big_dec,
                    small_pre=prefill_map[small_model],
                    small_dec=decode_map[small_model],
                    big_coverage=coverage,
                    chunk_size=256
                )
                multi_times.append(sim_result['total_time_s'])

            mean_multi_time = np.mean(multi_times)
            speedup_vs_big = big_only_time / mean_multi_time

            # Collect data
            coverage_list.append(1 - coverage)       # fraction of tokens done by big model
            speedup_list.append(speedup_vs_big)  # x-value

        # Now plot speedup (X) vs coverage (Y)
        plt.plot(
            speedup_list,
            coverage_list,
            label=f"Big: {big_model}",
            color=color,
            marker='o'
        )

    plt.title("Two-Model Pipelined Inference\nFraction Generated by Small vs. Speedup")
    plt.xlabel("Speedup vs Big-Only (X-axis)")
    plt.ylabel("Fraction Generated by Small Model (Y-axis)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("pipelined_simulation_inverted_axes.pdf", dpi=150)
    print("Saved figure to pipelined_simulation_inverted_axes.pdf")


def run_scaling_law_plot(
    coverage_start=0.0,
    coverage_end=0.30,
    coverage_step=0.01,
    sims_per_coverage=10,
    p_t=2048,
    d_t=16384
):
    # Prefill / decode rates for known models
    prefill_map = {
        "1.5B": 28868.4664,
        "7B": 7658.083194,
        "8B": 7456.973294,
        "14B": 3835.762802,
        "32B": 2594.868088,
        "70B": 2237.695188
    }
    decode_map = {
        "1.5B": 141,
        "7B": 41.3,
        "8B": 40.3,
        "14B": 22,
        "32B": 16,
        "70B": 9
    }

    # We'll fix the small model to 1.5B, vary the big model
    small_model = "1.5B"
    big_models = ["7B", "8B", "14B", "32B", "70B"]

    coverage_values = np.arange(coverage_start, coverage_end + 1e-9, coverage_step)

    # For pretty colormap
    cmap = plt.cm.CMRmap
    colors = cmap(np.linspace(0, 1, 2*len(big_models)))

    plt.figure(figsize=(8, 5))

    for color, big_model in zip(colors, big_models):
        speedup_vs_big_list = []

        # Single-model big-only baseline
        big_pre = prefill_map[big_model]
        big_dec = decode_map[big_model]
        big_only_time = (p_t / big_pre) + (d_t / big_dec)

        # For each coverage setting, do a set of simulations, compute mean time
        for coverage in coverage_values:
            # We'll run multiple times because the chunk assignment is random
            multi_times = []
            for _ in range(sims_per_coverage):
                sim_result = simulate_inference_time(
                    p_t=p_t,
                    d_t=d_t,
                    big_pre=big_pre,
                    big_dec=big_dec,
                    small_pre=prefill_map[small_model],
                    small_dec=decode_map[small_model],
                    big_coverage=coverage,
                    chunk_size=256
                )
                multi_times.append(sim_result['total_time_s'])

            mean_multi_time = np.mean(multi_times)

            # Speedup vs big = (big_only_time) / (mean_multi_time)
            speedup_vs_big = big_only_time / mean_multi_time
            speedup_vs_big_list.append(speedup_vs_big)

        # Plot coverage (%) vs speedup
        plt.plot(
            coverage_values * 100,
            speedup_vs_big_list,
            label=f"Big model: {big_model}",
            color=color,
            # marker='o'
        )

    plt.title("Two-Model Pipelined Inference: Speedup vs Big-Only")
    plt.xlabel("Big-Model Offload (%)")
    plt.ylabel("Speedup vs Big-Only")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("pipelined_simulation.pdf")
    print("Saved figure to pipelined_simulation.pdf")


def parse_args():
    parser = argparse.ArgumentParser(description="Simulate inference time for two models.")
    parser.add_argument('--big_model', type=str, default="14B", help='Size')
    parser.add_argument('--small_model', type=str, default="1.5B", help='Size')
    parser.add_argument('--big_coverage', type=float, default=0.1, help='Fraction of decode tokens covered by the big model')
    parser.add_argument('--chunk_size', type=int, default=256, help='Size of each chunk')
    parser.add_argument('--prefill_size', type=int, default=2048, help='Prefill length')
    parser.add_argument('--decode_size', type=int, default=16384, help='Decode length')
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()

    # Prefill tok/sec
    prefill_map = {
        "1.5B": 28868.4664,
        "7B": 7658.083194,
        "8B": 7456.973294,
        "14B": 3835.762802,
        "32B": 2594.868088,
        "70B": 2237.695188
    }
    # Decode tok/s
    decode_map = {
        "1.5B": 141,
        "7B": 41.3,
        "8B": 40.3,
        "14B": 22,
        "32B": 16,
        "70B": 9
    }

    p_t = args.prefill_size
    d_t = args.decode_size
    big_pre = prefill_map[args.big_model]
    small_pre = prefill_map[args.small_model]
    big_dec = decode_map[args.big_model]
    small_dec = decode_map[args.small_model]
    big_coverage = args.big_coverage
    chunk_size = args.chunk_size

    # random.seed(42)  # For reproducibility

    # --- Simulate Two-Model Approach multiple times ---
    simulations = []
    for _ in range(10):
        sim_result = simulate_inference_time(
            p_t=p_t,
            d_t=d_t,
            big_pre=big_pre,
            small_pre=small_pre,
            big_dec=big_dec,
            small_dec=small_dec,
            big_coverage=big_coverage,
            chunk_size=chunk_size
        )
        simulations.append(sim_result)

    # Analyze multi-model approach
    multi_times = [sim['total_time_s'] for sim in simulations]
    median_multi = np.median(multi_times)
    mean_multi = np.mean(multi_times)
    std_multi = np.std(multi_times)

    # --- Single-Model: Big-Only ---
    # Prefill + decode, sequentially (since there's only one model)
    big_only_time = (p_t / big_pre) + (d_t / big_dec)

    # --- Single-Model: Small-Only ---
    small_only_time = (p_t / small_pre) + (d_t / small_dec)

    # Speeds in tokens/s: total_tokens / total_time
    # total_tokens = p_t + d_t
    total_tokens = p_t + d_t

    big_only_speed = total_tokens / big_only_time
    small_only_speed = total_tokens / small_only_time
    multi_mean_speed = total_tokens / mean_multi
    multi_median_speed = total_tokens / median_multi

    # Speedups: (comparing times, smaller time => faster)
    # e.g. speedup vs big = big_only_time / multi_time
    speedup_vs_big_mean = big_only_time / mean_multi
    speedup_vs_small_mean = small_only_time / mean_multi
    speedup_vs_big_median = big_only_time / median_multi
    speedup_vs_small_median = small_only_time / median_multi

    print("==== Arguments ====")
    print(f"Big Model: {args.big_model}")
    print(f"Small Model: {args.small_model}")
    print(f"Big Coverage: {big_coverage}")
    print(f"Chunk Size: {chunk_size}")
    print(f"Prefill Size: {p_t}, Decode Size: {d_t}")
    print()

    print("==== Two-Model Simulations ====")
    print(f"# of simulations: {len(simulations)}")
    print(f"Mean Two-Model Time (s):    {mean_multi:.3f}")
    print(f"Median Two-Model Time (s):  {median_multi:.3f}")
    print(f"Std Dev Two-Model Time (s): {std_multi:.3f}")
    print()

    print("==== Single-Model Times ====")
    print(f"Big-Only Time (s):   {big_only_time:.3f}")
    print(f"Small-Only Time (s): {small_only_time:.3f}")
    print()

    print("==== Speeds (tok/s) ====")
    print(f"Big-Only Speed:        {big_only_speed:.3f}")
    print(f"Small-Only Speed:      {small_only_speed:.3f}")
    print(f"Two-Model Mean Speed:  {multi_mean_speed:.3f}")
    print(f"Two-Model Median Speed:{multi_median_speed:.3f}")
    print()

    print("==== Speedup Comparisons ====")
    print("  (Speedup vs Big = BigOnlyTime / MultiTime: how many times faster is the two-model approach than Big-Only)")
    print("  (Speedup vs Small = SmallOnlyTime / MultiTime: how many times faster is the two-model approach than Small-Only)")
    print(f"Mean-based Speedup vs Big:    {speedup_vs_big_mean:.3f}")
    print(f"Mean-based Speedup vs Small:  {speedup_vs_small_mean:.3f}")
    print(f"Median-based Speedup vs Big:  {speedup_vs_big_median:.3f}")
    print(f"Median-based Speedup vs Small:{speedup_vs_small_median:.3f}")
    run_scaling_law_plot()
    run_inverted_scaling_law_plot()
